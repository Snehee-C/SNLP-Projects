{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-16T17:13:55.279596Z","iopub.execute_input":"2024-09-16T17:13:55.279882Z","iopub.status.idle":"2024-09-16T17:13:55.635327Z","shell.execute_reply.started":"2024-09-16T17:13:55.279848Z","shell.execute_reply":"2024-09-16T17:13:55.634410Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/pii-detection-removal-from-educational-data/sample_submission.csv\n/kaggle/input/pii-detection-removal-from-educational-data/train.json\n/kaggle/input/pii-detection-removal-from-educational-data/test.json\n","output_type":"stream"}]},{"cell_type":"code","source":"TRAINING_MODEL_PATH = \"microsoft/deberta-v3-base\"\nTRAINING_MAX_LENGTH = 1024\nOUTPUT_DIR = \"output\"","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:13:55.637166Z","iopub.execute_input":"2024-09-16T17:13:55.637949Z","iopub.status.idle":"2024-09-16T17:13:55.641769Z","shell.execute_reply.started":"2024-09-16T17:13:55.637900Z","shell.execute_reply":"2024-09-16T17:13:55.640887Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install seqeval evaluate -q","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:13:55.643194Z","iopub.execute_input":"2024-09-16T17:13:55.643531Z","iopub.status.idle":"2024-09-16T17:14:13.410363Z","shell.execute_reply.started":"2024-09-16T17:13:55.643491Z","shell.execute_reply":"2024-09-16T17:14:13.409263Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import json\nimport pandas as pd\nimport numpy as np\nimport argparse\nfrom itertools import chain\nfrom functools import partial\nimport torch\nfrom transformers import AutoTokenizer, Trainer, TrainingArguments\nfrom transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification\nimport evaluate\nfrom datasets import Dataset, features\nfrom seqeval.metrics import recall_score, precision_score\nfrom seqeval.metrics import classification_report\nfrom seqeval.metrics import f1_score\nfrom pathlib import Path","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:49:05.745643Z","iopub.execute_input":"2024-09-16T17:49:05.746050Z","iopub.status.idle":"2024-09-16T17:49:05.752405Z","shell.execute_reply.started":"2024-09-16T17:49:05.746000Z","shell.execute_reply":"2024-09-16T17:49:05.751362Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"train_data = json.load(open('/kaggle/input/pii-detection-removal-from-educational-data/train.json'))","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:14:32.560385Z","iopub.execute_input":"2024-09-16T17:14:32.560929Z","iopub.status.idle":"2024-09-16T17:14:35.152399Z","shell.execute_reply.started":"2024-09-16T17:14:32.560893Z","shell.execute_reply":"2024-09-16T17:14:35.151372Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in train_data]))))\nlabel2id = {l: i for i,l in enumerate(all_labels)}\nid2label = {v:k for k,v in label2id.items()}\n\ntarget = [item for item in all_labels if item != 'O']\n\nprint(id2label)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:14:35.153640Z","iopub.execute_input":"2024-09-16T17:14:35.154072Z","iopub.status.idle":"2024-09-16T17:14:35.232626Z","shell.execute_reply.started":"2024-09-16T17:14:35.154002Z","shell.execute_reply":"2024-09-16T17:14:35.231793Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"{0: 'B-EMAIL', 1: 'B-ID_NUM', 2: 'B-NAME_STUDENT', 3: 'B-PHONE_NUM', 4: 'B-STREET_ADDRESS', 5: 'B-URL_PERSONAL', 6: 'B-USERNAME', 7: 'I-ID_NUM', 8: 'I-NAME_STUDENT', 9: 'I-PHONE_NUM', 10: 'I-STREET_ADDRESS', 11: 'I-URL_PERSONAL', 12: 'O'}\n","output_type":"stream"}]},{"cell_type":"code","source":"def rebuild_text(data):\n    \n    text, labels = [], []\n    \n    for tok, lab, ws in zip(\n        data[\"tokens\"], data[\"provided_labels\"], data[\"trailing_whitespace\"]\n    ):\n        # append each token to the reconstructed text and the label for each token's character\n        text.append(tok)\n        labels.extend([lab] * len(tok))\n        \n        # add space in text if whitespace and label \"O\"\n        if ws:\n            text.append(\" \")\n            labels.append(\"O\")\n            \n    return text, labels","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:14:35.234108Z","iopub.execute_input":"2024-09-16T17:14:35.234451Z","iopub.status.idle":"2024-09-16T17:14:35.409556Z","shell.execute_reply.started":"2024-09-16T17:14:35.234413Z","shell.execute_reply":"2024-09-16T17:14:35.408387Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def tokenize(data, tokenizer, label2id, max_length):\n    \n    text, labels = rebuild_text(data)\n    text = \"\".join(text)\n    labels = np.array(labels)\n    token_labels = []\n    \n    # returns a dictionary-like object containing tokenized inputs and offsets mapping (represents the mapping between the tokens and their corresponding positions in the original text)\n    tokenized = tokenizer(text, return_offsets_mapping=True, max_length=max_length)\n    \n    for start_idx, end_idx in tokenized.offset_mapping:\n        \n        # if CLS tokens\n        if start_idx == 0 and end_idx == 0:\n            token_labels.append(label2id[\"O\"])\n            continue\n            \n        # if token starts with ws\n        if text[start_idx].isspace():\n            start_idx += 1\n            \n        token_labels.append(label2id[labels[start_idx]])\n        \n    length = len(tokenized.input_ids)\n\n    return {**tokenized, \"labels\": token_labels, \"length\": length}","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:14:35.411077Z","iopub.execute_input":"2024-09-16T17:14:35.411544Z","iopub.status.idle":"2024-09-16T17:14:35.449491Z","shell.execute_reply.started":"2024-09-16T17:14:35.411499Z","shell.execute_reply":"2024-09-16T17:14:35.448549Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(TRAINING_MODEL_PATH)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:14:35.450752Z","iopub.execute_input":"2024-09-16T17:14:35.451062Z","iopub.status.idle":"2024-09-16T17:14:38.147895Z","shell.execute_reply.started":"2024-09-16T17:14:35.451027Z","shell.execute_reply":"2024-09-16T17:14:38.146789Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"166ae75e5094477b939ed0a10e5f4ac3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"936a6a4358c9494fa36427ae042179ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fee1df76453f4130b679883eb84624cb"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"ds = Dataset.from_dict({\n    \"full_text\": [x[\"full_text\"] for x in train_data],\n    \"document\": [str(x[\"document\"]) for x in train_data],\n    \"tokens\": [x[\"tokens\"] for x in train_data],\n    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in train_data],\n    \"provided_labels\": [x[\"labels\"] for x in train_data],\n})","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:14:38.151777Z","iopub.execute_input":"2024-09-16T17:14:38.152119Z","iopub.status.idle":"2024-09-16T17:14:40.220052Z","shell.execute_reply.started":"2024-09-16T17:14:38.152085Z","shell.execute_reply":"2024-09-16T17:14:40.219274Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"ds = ds.map(tokenize, fn_kwargs={\"tokenizer\":tokenizer, \"label2id\":label2id, \"max_length\":TRAINING_MAX_LENGTH}, num_proc=3)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:14:40.221034Z","iopub.execute_input":"2024-09-16T17:14:40.221319Z","iopub.status.idle":"2024-09-16T17:15:13.185084Z","shell.execute_reply.started":"2024-09-16T17:14:40.221289Z","shell.execute_reply":"2024-09-16T17:15:13.184207Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=3):   0%|          | 0/6807 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d64194feba64dd191f1b35f7ad1fa0f"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Compare tokens and labels for original dataset and new tokenization\nx = ds[0]\n\nfor t,l in zip(x[\"tokens\"], x[\"provided_labels\"]):\n    if l != \"O\":\n        print((t,l))\n\nprint(\"*\"*100)\n\nfor t, l in zip(tokenizer.convert_ids_to_tokens(x[\"input_ids\"]), x[\"labels\"]):\n    if id2label[l] != \"O\":\n        print((t,id2label[l]))","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:15:13.186617Z","iopub.execute_input":"2024-09-16T17:15:13.186990Z","iopub.status.idle":"2024-09-16T17:15:13.205318Z","shell.execute_reply.started":"2024-09-16T17:15:13.186937Z","shell.execute_reply":"2024-09-16T17:15:13.204305Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"('Nathalie', 'B-NAME_STUDENT')\n('Sylla', 'I-NAME_STUDENT')\n('Nathalie', 'B-NAME_STUDENT')\n('Sylla', 'I-NAME_STUDENT')\n('Nathalie', 'B-NAME_STUDENT')\n('Sylla', 'I-NAME_STUDENT')\n****************************************************************************************************\n('N', 'B-NAME_STUDENT')\n('atha', 'B-NAME_STUDENT')\n('lie', 'B-NAME_STUDENT')\n('â–S', 'I-NAME_STUDENT')\n('ylla', 'I-NAME_STUDENT')\n('N', 'B-NAME_STUDENT')\n('atha', 'B-NAME_STUDENT')\n('lie', 'B-NAME_STUDENT')\n('â–S', 'I-NAME_STUDENT')\n('ylla', 'I-NAME_STUDENT')\n('N', 'B-NAME_STUDENT')\n('atha', 'B-NAME_STUDENT')\n('lie', 'B-NAME_STUDENT')\n('â–S', 'I-NAME_STUDENT')\n('ylla', 'I-NAME_STUDENT')\n","output_type":"stream"}]},{"cell_type":"code","source":"def compute_metrics(p, all_labels):\n    # p is a tuple containing preds and true labels\n    predictions, labels = p\n    # preds are in form of probs for each label for each token => we take the highest one\n    predictions = np.argmax(predictions, axis=2)\n\n    # Remove special tokens from preds and labels\n    true_predictions = [\n        [all_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    \n    true_labels = [\n        [all_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    \n    # Compute metrics using sklearn and own formula\n    recall = recall_score(true_labels, true_predictions)\n    precision = precision_score(true_labels, true_predictions)\n    f1_score = (1 + 5*5) * recall * precision / (5*5*precision + recall)\n    \n    # Store metrics and return\n    results = {\n        'recall': recall,\n        'precision': precision,\n        'f1': f1_score\n    }\n    \n    return results","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:15:13.206444Z","iopub.execute_input":"2024-09-16T17:15:13.206734Z","iopub.status.idle":"2024-09-16T17:15:13.233713Z","shell.execute_reply.started":"2024-09-16T17:15:13.206702Z","shell.execute_reply":"2024-09-16T17:15:13.232916Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForTokenClassification.from_pretrained(\n    TRAINING_MODEL_PATH,\n    num_labels=len(all_labels),\n    id2label=id2label,\n    label2id=label2id,\n    ignore_mismatched_sizes=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:15:13.235102Z","iopub.execute_input":"2024-09-16T17:15:13.235399Z","iopub.status.idle":"2024-09-16T17:15:17.004234Z","shell.execute_reply.started":"2024-09-16T17:15:13.235368Z","shell.execute_reply":"2024-09-16T17:15:17.003365Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"940cc88d722441329d5e4f16a4c8ea9a"}},"metadata":{}},{"name":"stderr","text":"Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:15:17.005683Z","iopub.execute_input":"2024-09-16T17:15:17.006236Z","iopub.status.idle":"2024-09-16T17:15:17.010904Z","shell.execute_reply.started":"2024-09-16T17:15:17.006192Z","shell.execute_reply":"2024-09-16T17:15:17.009945Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Define training arguments\nargs = TrainingArguments(\n    output_dir=OUTPUT_DIR, \n    fp16=True,\n    learning_rate=2e-5,\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=2,\n    report_to=\"none\",\n    evaluation_strategy=\"no\",\n    do_eval=False,\n    save_total_limit=1,\n    logging_steps=20,\n    lr_scheduler_type='cosine',\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    warmup_ratio=0.1,\n    weight_decay=0.01\n)\n\ntrainer = Trainer(\n    model=model, \n    args=args, \n    train_dataset=ds,\n    data_collator=collator, \n    tokenizer=tokenizer,\n    compute_metrics=partial(compute_metrics, all_labels=all_labels),\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:15:17.012082Z","iopub.execute_input":"2024-09-16T17:15:17.012783Z","iopub.status.idle":"2024-09-16T17:15:17.508646Z","shell.execute_reply.started":"2024-09-16T17:15:17.012741Z","shell.execute_reply":"2024-09-16T17:15:17.507602Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:15:17.509923Z","iopub.execute_input":"2024-09-16T17:15:17.510254Z","iopub.status.idle":"2024-09-16T17:38:18.837532Z","shell.execute_reply.started":"2024-09-16T17:15:17.510221Z","shell.execute_reply":"2024-09-16T17:38:18.836686Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='425' max='425' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [425/425 22:56, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>2.238200</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.166200</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.006300</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.008500</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.006500</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.004100</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.006100</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.001800</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.004800</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.002100</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.003900</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.002200</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.001400</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.002700</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.001400</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.001200</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.001800</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.002200</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.001700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=425, training_loss=0.11601698581512798, metrics={'train_runtime': 1380.615, 'train_samples_per_second': 4.93, 'train_steps_per_second': 0.308, 'total_flos': 3406207463440128.0, 'train_loss': 0.11601698581512798, 'epoch': 0.9988249118683902})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.save_model(\"deberta3base_1024\")\ntokenizer.save_pretrained(\"deberta3base_1024\")","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:38:18.838652Z","iopub.execute_input":"2024-09-16T17:38:18.838974Z","iopub.status.idle":"2024-09-16T17:38:20.317507Z","shell.execute_reply.started":"2024-09-16T17:38:18.838937Z","shell.execute_reply":"2024-09-16T17:38:20.316429Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"('deberta3base_1024/tokenizer_config.json',\n 'deberta3base_1024/special_tokens_map.json',\n 'deberta3base_1024/spm.model',\n 'deberta3base_1024/added_tokens.json',\n 'deberta3base_1024/tokenizer.json')"},"metadata":{}}]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"INFERENCE_MAX_LENGTH = 2048\nmodel_path = '/kaggle/working/deberta3base_1024'","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:45:28.732687Z","iopub.execute_input":"2024-09-16T17:45:28.733698Z","iopub.status.idle":"2024-09-16T17:45:28.737804Z","shell.execute_reply.started":"2024-09-16T17:45:28.733654Z","shell.execute_reply":"2024-09-16T17:45:28.736803Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"test_data = json.load(open(\"/kaggle/input/pii-detection-removal-from-educational-data/test.json\"))","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:45:40.153426Z","iopub.execute_input":"2024-09-16T17:45:40.153925Z","iopub.status.idle":"2024-09-16T17:45:40.186769Z","shell.execute_reply.started":"2024-09-16T17:45:40.153881Z","shell.execute_reply":"2024-09-16T17:45:40.186029Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def tokenize_test(data, tokenizer):\n    text, token_map = [], []\n    idx = 0\n    for tok, ws in zip(data[\"tokens\"], data[\"trailing_whitespace\"]):\n        text.append(tok)\n        token_map.extend([idx] * len(tok))\n        if ws:\n            text.append(\" \")\n            token_map.append(-1)\n        idx += 1\n    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=INFERENCE_MAX_LENGTH)\n    return {**tokenized, \"token_map\": token_map}","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:46:46.116393Z","iopub.execute_input":"2024-09-16T17:46:46.116795Z","iopub.status.idle":"2024-09-16T17:46:46.123731Z","shell.execute_reply.started":"2024-09-16T17:46:46.116758Z","shell.execute_reply":"2024-09-16T17:46:46.122663Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"ds = Dataset.from_dict({\n    \"full_text\": [x[\"full_text\"] for x in test_data],\n    \"document\": [x[\"document\"] for x in test_data],\n    \"tokens\": [x[\"tokens\"] for x in test_data],\n    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in test_data],\n})","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:47:00.372504Z","iopub.execute_input":"2024-09-16T17:47:00.372893Z","iopub.status.idle":"2024-09-16T17:47:00.395100Z","shell.execute_reply.started":"2024-09-16T17:47:00.372854Z","shell.execute_reply":"2024-09-16T17:47:00.394210Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_path)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:47:17.035010Z","iopub.execute_input":"2024-09-16T17:47:17.035742Z","iopub.status.idle":"2024-09-16T17:47:17.244093Z","shell.execute_reply.started":"2024-09-16T17:47:17.035702Z","shell.execute_reply":"2024-09-16T17:47:17.243260Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"ds = ds.map(tokenize_test, fn_kwargs={\"tokenizer\": tokenizer}, num_proc=2)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:47:39.799753Z","iopub.execute_input":"2024-09-16T17:47:39.800467Z","iopub.status.idle":"2024-09-16T17:47:40.697725Z","shell.execute_reply.started":"2024-09-16T17:47:39.800425Z","shell.execute_reply":"2024-09-16T17:47:40.696833Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4cc6d2ee9fd4841850a25a06e33bfd3"}},"metadata":{}}]},{"cell_type":"code","source":"model = AutoModelForTokenClassification.from_pretrained(model_path)\ncollator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\nargs = TrainingArguments(\n    \".\", \n    per_device_eval_batch_size=1, \n    report_to=\"none\",\n)\n\ntrainer = Trainer(\n    model=model, \n    args=args, \n    data_collator=collator, \n    tokenizer=tokenizer,\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:48:14.280398Z","iopub.execute_input":"2024-09-16T17:48:14.281253Z","iopub.status.idle":"2024-09-16T17:48:14.634501Z","shell.execute_reply.started":"2024-09-16T17:48:14.281201Z","shell.execute_reply":"2024-09-16T17:48:14.633691Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"}]},{"cell_type":"code","source":"predictions = trainer.predict(ds).predictions\npred_softmax = np.exp(predictions) / np.sum(np.exp(predictions), axis = 2).reshape(predictions.shape[0],predictions.shape[1],1)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:48:34.542947Z","iopub.execute_input":"2024-09-16T17:48:34.543883Z","iopub.status.idle":"2024-09-16T17:48:35.875725Z","shell.execute_reply.started":"2024-09-16T17:48:34.543840Z","shell.execute_reply":"2024-09-16T17:48:35.874734Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":"config = json.load(open(Path(model_path) / \"config.json\"))\nid2label = config[\"id2label\"]","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:49:10.685545Z","iopub.execute_input":"2024-09-16T17:49:10.686323Z","iopub.status.idle":"2024-09-16T17:49:10.691086Z","shell.execute_reply.started":"2024-09-16T17:49:10.686281Z","shell.execute_reply":"2024-09-16T17:49:10.690053Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"preds = predictions.argmax(-1)\npreds_without_O = pred_softmax[:,:,:12].argmax(-1)\nO_preds = pred_softmax[:,:,12]","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:49:27.256789Z","iopub.execute_input":"2024-09-16T17:49:27.257424Z","iopub.status.idle":"2024-09-16T17:49:27.263839Z","shell.execute_reply.started":"2024-09-16T17:49:27.257379Z","shell.execute_reply":"2024-09-16T17:49:27.263034Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"threshold = 0.9\npreds_final = np.where(O_preds < threshold, preds_without_O, preds)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:49:37.634258Z","iopub.execute_input":"2024-09-16T17:49:37.634638Z","iopub.status.idle":"2024-09-16T17:49:37.639574Z","shell.execute_reply.started":"2024-09-16T17:49:37.634606Z","shell.execute_reply":"2024-09-16T17:49:37.638652Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"triplets = []\ndocument, token, label, token_str = [], [], [], []","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:49:47.530545Z","iopub.execute_input":"2024-09-16T17:49:47.530923Z","iopub.status.idle":"2024-09-16T17:49:47.535590Z","shell.execute_reply.started":"2024-09-16T17:49:47.530886Z","shell.execute_reply":"2024-09-16T17:49:47.534492Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"for p, token_map, offsets, tokens, doc in zip(preds_final, ds[\"token_map\"], ds[\"offset_mapping\"], ds[\"tokens\"], ds[\"document\"]):\n\n    for token_pred, (start_idx, end_idx) in zip(p, offsets):\n        label_pred = id2label[str(token_pred)]\n\n        if start_idx + end_idx == 0: continue\n\n        if token_map[start_idx] == -1:\n            start_idx += 1\n\n        while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n            start_idx += 1\n\n        if start_idx >= len(token_map): break\n        \n        token_id = token_map[start_idx]\n\n        if label_pred != \"O\" and token_id != -1:\n            triplet = (label_pred, token_id, tokens[token_id])\n\n            if triplet not in triplets:\n                document.append(doc)\n                token.append(token_id)\n                label.append(label_pred)\n                token_str.append(tokens[token_id])\n                triplets.append(triplet)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:50:07.051769Z","iopub.execute_input":"2024-09-16T17:50:07.052621Z","iopub.status.idle":"2024-09-16T17:50:07.145288Z","shell.execute_reply.started":"2024-09-16T17:50:07.052578Z","shell.execute_reply":"2024-09-16T17:50:07.144546Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame({\n    \"document\": document,\n    \"token\": token,\n    \"label\": label,\n    \"token_str\": token_str\n})\ndf[\"row_id\"] = list(range(len(df)))","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:50:20.116333Z","iopub.execute_input":"2024-09-16T17:50:20.116710Z","iopub.status.idle":"2024-09-16T17:50:20.128857Z","shell.execute_reply.started":"2024-09-16T17:50:20.116672Z","shell.execute_reply":"2024-09-16T17:50:20.127925Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"df.head(30)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T17:50:27.431697Z","iopub.execute_input":"2024-09-16T17:50:27.432097Z","iopub.status.idle":"2024-09-16T17:50:27.451356Z","shell.execute_reply.started":"2024-09-16T17:50:27.432056Z","shell.execute_reply":"2024-09-16T17:50:27.450372Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"    document  token           label   token_str  row_id\n0          7      9  B-NAME_STUDENT    Nathalie       0\n1          7     10  I-NAME_STUDENT       Sylla       1\n2          7    482  B-NAME_STUDENT    Nathalie       2\n3          7    483  I-NAME_STUDENT       Sylla       3\n4          7    741  B-NAME_STUDENT    Nathalie       4\n5          7    742  I-NAME_STUDENT       Sylla       5\n6         10      0  B-NAME_STUDENT       Diego       6\n7         10      1  I-NAME_STUDENT     Estrada       7\n8         10    464  B-NAME_STUDENT       Diego       8\n9         10    465  I-NAME_STUDENT     Estrada       9\n10        16      4  B-NAME_STUDENT    Gilberto      10\n11        16      5  I-NAME_STUDENT      Gamboa      11\n12        20      5  B-NAME_STUDENT       Sindy      12\n13        20      6  I-NAME_STUDENT      Samaca      13\n14        20      8  I-NAME_STUDENT       Gitam      14\n15        56     12  B-NAME_STUDENT      Nadine      15\n16        56     13  I-NAME_STUDENT        Born      16\n17        86      6  B-NAME_STUDENT      Eladio      17\n18        86      7  I-NAME_STUDENT       Amaya      18\n19        93      0  B-NAME_STUDENT      Silvia      19\n20        93      1  I-NAME_STUDENT  Villalobos      20\n21       104      7  B-NAME_STUDENT          Dr      21\n22       104      8  B-NAME_STUDENT       Sakir      22\n23       104      9  I-NAME_STUDENT       Ahmad      23\n24       112      5  B-NAME_STUDENT   Francisco      24\n25       112      6  I-NAME_STUDENT    Ferreira      25\n26       123     32  B-NAME_STUDENT     Stefano      26\n27       123     33  I-NAME_STUDENT      Lovato      27\n28       123     38  B-NAME_STUDENT  Sathyabama      28\n29       123     38  I-NAME_STUDENT  Sathyabama      29","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>document</th>\n      <th>token</th>\n      <th>label</th>\n      <th>token_str</th>\n      <th>row_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7</td>\n      <td>9</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Nathalie</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7</td>\n      <td>10</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Sylla</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7</td>\n      <td>482</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Nathalie</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7</td>\n      <td>483</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Sylla</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>741</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Nathalie</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>7</td>\n      <td>742</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Sylla</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>10</td>\n      <td>0</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Diego</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>10</td>\n      <td>1</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Estrada</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>10</td>\n      <td>464</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Diego</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>465</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Estrada</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>16</td>\n      <td>4</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Gilberto</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>16</td>\n      <td>5</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Gamboa</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>20</td>\n      <td>5</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Sindy</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>20</td>\n      <td>6</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Samaca</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>20</td>\n      <td>8</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Gitam</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>56</td>\n      <td>12</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Nadine</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>56</td>\n      <td>13</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Born</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>86</td>\n      <td>6</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Eladio</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>86</td>\n      <td>7</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Amaya</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>93</td>\n      <td>0</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Silvia</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>93</td>\n      <td>1</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Villalobos</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>104</td>\n      <td>7</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Dr</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>104</td>\n      <td>8</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Sakir</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>104</td>\n      <td>9</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Ahmad</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>112</td>\n      <td>5</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Francisco</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>112</td>\n      <td>6</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Ferreira</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>123</td>\n      <td>32</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Stefano</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>123</td>\n      <td>33</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Lovato</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>123</td>\n      <td>38</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Sathyabama</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>123</td>\n      <td>38</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Sathyabama</td>\n      <td>29</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}