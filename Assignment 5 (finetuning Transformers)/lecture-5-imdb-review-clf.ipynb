{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9292235,"sourceType":"datasetVersion","datasetId":5625493}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-01T06:14:12.072423Z","iopub.execute_input":"2024-09-01T06:14:12.072723Z","iopub.status.idle":"2024-09-01T06:14:12.433527Z","shell.execute_reply.started":"2024-09-01T06:14:12.072690Z","shell.execute_reply":"2024-09-01T06:14:12.432569Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nimport transformers\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn import model_selection, metrics","metadata":{"execution":{"iopub.status.busy":"2024-09-01T06:17:17.494936Z","iopub.execute_input":"2024-09-01T06:17:17.495835Z","iopub.status.idle":"2024-09-01T06:17:22.417668Z","shell.execute_reply.started":"2024-09-01T06:17:17.495787Z","shell.execute_reply":"2024-09-01T06:17:22.416603Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n* Dataset class\n* Model\n* Trainer - training arguments\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-09-01T06:14:12.435270Z","iopub.execute_input":"2024-09-01T06:14:12.435653Z","iopub.status.idle":"2024-09-01T06:14:12.442135Z","shell.execute_reply.started":"2024-09-01T06:14:12.435620Z","shell.execute_reply":"2024-09-01T06:14:12.441253Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'\\n* Dataset class\\n* Model\\n* Trainer - training arguments\\n'"},"metadata":{}}]},{"cell_type":"code","source":"config = {\n    \"max_length\": 360,\n    \"model_path\": \"microsoft/xtremedistil-l6-h256-uncased\",\n\n    \"output_dir\": \"./my-model\",\n    \"train_batch_size\": 64,\n    \"valid_batch_size\": 64,\n    \"learning_rate\": 3e-5,\n    \"epochs\": 3,\n\n    \"debug\": True,\n}","metadata":{"execution":{"iopub.status.busy":"2024-09-01T06:14:12.443316Z","iopub.execute_input":"2024-09-01T06:14:12.443673Z","iopub.status.idle":"2024-09-01T06:14:12.453226Z","shell.execute_reply.started":"2024-09-01T06:14:12.443629Z","shell.execute_reply":"2024-09-01T06:14:12.452299Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"tokenizer = transformers.AutoTokenizer.from_pretrained(config[\"model_path\"])\nclass TextDataset:\n\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return self.data.shape[0]\n\n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n\n        enc = enc = tokenizer(\n            row[\"text\"],\n            add_special_tokens=True,\n            max_length=config[\"max_length\"],\n            padding=\"max_length\",\n            truncation=True\n        )\n\n        return {\n            \"input_ids\": torch.tensor(enc[\"input_ids\"]),\n            \"attention_mask\": torch.tensor(enc[\"attention_mask\"]),\n            \"label\": torch.tensor(row[\"label\"]),\n        }","metadata":{"execution":{"iopub.status.busy":"2024-09-01T06:21:20.617266Z","iopub.execute_input":"2024-09-01T06:21:20.617664Z","iopub.status.idle":"2024-09-01T06:21:20.898370Z","shell.execute_reply.started":"2024-09-01T06:21:20.617628Z","shell.execute_reply":"2024-09-01T06:21:20.897364Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/imdb-data/IMDB Dataset 3.csv\").rename(columns={\"review\": \"text\"})\n\nid2label = {0: \"negative\", 1: \"positive\"}\nlabel2id = {label: id_ for id_, label in id2label.items()}\n\ndf[\"label\"] = df[\"sentiment\"].map(label2id)\n\nif config[\"debug\"]:\n    print(\"DEBUG MODE!\")\n    df = df.sample(10_000, random_state=123)\n\nprint(df.shape)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-01T06:21:23.533822Z","iopub.execute_input":"2024-09-01T06:21:23.534252Z","iopub.status.idle":"2024-09-01T06:21:24.287324Z","shell.execute_reply.started":"2024-09-01T06:21:23.534212Z","shell.execute_reply":"2024-09-01T06:21:24.286325Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"DEBUG MODE!\n(10000, 3)\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"                                                    text sentiment  label\n11872  This movie was beyond awful, it was a pimple o...  negative      0\n40828  As of this writing John Carpenter's 'Halloween...  positive      1\n36400  I must admit a slight disappointment with this...  positive      1\n5166   Oh dear! The BBC is not about to be knocked of...  negative      0\n30273  its a totally average film with a few semi-alr...  negative      0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>sentiment</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>11872</th>\n      <td>This movie was beyond awful, it was a pimple o...</td>\n      <td>negative</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>40828</th>\n      <td>As of this writing John Carpenter's 'Halloween...</td>\n      <td>positive</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>36400</th>\n      <td>I must admit a slight disappointment with this...</td>\n      <td>positive</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5166</th>\n      <td>Oh dear! The BBC is not about to be knocked of...</td>\n      <td>negative</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>30273</th>\n      <td>its a totally average film with a few semi-alr...</td>\n      <td>negative</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = transformers.AutoTokenizer.from_pretrained(config[\"model_path\"])","metadata":{"execution":{"iopub.status.busy":"2024-09-01T06:21:25.423065Z","iopub.execute_input":"2024-09-01T06:21:25.423468Z","iopub.status.idle":"2024-09-01T06:21:25.769612Z","shell.execute_reply.started":"2024-09-01T06:21:25.423431Z","shell.execute_reply":"2024-09-01T06:21:25.768627Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"train, valid = model_selection.train_test_split(\n    df,\n    test_size=0.2,\n    random_state=23,\n    shuffle=True,\n    stratify=df[\"label\"]\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T06:21:26.861494Z","iopub.execute_input":"2024-09-01T06:21:26.862266Z","iopub.status.idle":"2024-09-01T06:21:26.874044Z","shell.execute_reply.started":"2024-09-01T06:21:26.862226Z","shell.execute_reply":"2024-09-01T06:21:26.873149Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"train_ds = TextDataset(train)\nvalid_ds = TextDataset(valid)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T06:21:28.062507Z","iopub.execute_input":"2024-09-01T06:21:28.063443Z","iopub.status.idle":"2024-09-01T06:21:28.067804Z","shell.execute_reply.started":"2024-09-01T06:21:28.063399Z","shell.execute_reply":"2024-09-01T06:21:28.066851Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"valid_ds[0]","metadata":{"execution":{"iopub.status.busy":"2024-09-01T06:21:29.126416Z","iopub.execute_input":"2024-09-01T06:21:29.126791Z","iopub.status.idle":"2024-09-01T06:21:29.187592Z","shell.execute_reply.started":"2024-09-01T06:21:29.126754Z","shell.execute_reply":"2024-09-01T06:21:29.186595Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([  101, 18224,  4735,  5760,  2011,  5529,  8040, 10222, 11705,  1010,\n          2040,  2003,  3161,  1999,  5365,  1010,  2066,  3680,  2061, 10421,\n         16089, 15992,  1012,  2175, 26327,  2577,  1998,  7658,  5267, 22770,\n         13542,  2063,  2024, 11065,  1996, 21027,  1005,  1055,  2197,  3521,\n          1997,  1055, 11231, 10177,  4757,  1012,  2027,  2024,  2205,  2583,\n          1010,  2129,  2064,  1045,  2131,  5475,  2046,  1037,  6336,  2279,\n          2051,  1029,   102,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n 'label': tensor(1)}"},"metadata":{}}]},{"cell_type":"code","source":"model = transformers.AutoModelForSequenceClassification.from_pretrained(config[\"model_path\"])\n","metadata":{"execution":{"iopub.status.busy":"2024-09-01T06:21:54.179006Z","iopub.execute_input":"2024-09-01T06:21:54.179888Z","iopub.status.idle":"2024-09-01T06:21:56.607388Z","shell.execute_reply.started":"2024-09-01T06:21:54.179837Z","shell.execute_reply":"2024-09-01T06:21:56.606604Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/51.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6540bd6b3b8c4f739d81442e9150515e"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/xtremedistil-l6-h256-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"def compute_metrics(eval_data):\n   \n    preds = eval_data.predictions.argmax(-1)\n    labels = eval_data.label_ids \n    print(eval_data)\n    print(preds)\n    print(labels)\n\n    return {\n        'accuracy': metrics.accuracy_score(labels, preds),\n        'precision': metrics.precision_score(labels, preds),\n        'recall': metrics.recall_score(labels, preds),\n        'classification_report': metrics.classification_report(labels, preds, target_names=list(id2label.values()), output_dict=True)\n\n    }\n\ntraining_args = transformers.TrainingArguments(\n     output_dir=\"./results\",                      # Directory for storing results\n    evaluation_strategy=\"steps\",                 # Evaluate every few steps\n    per_device_train_batch_size=config['train_batch_size'],              # Batch size per device during training\n    per_device_eval_batch_size=config['train_batch_size'],               # Batch size per device during evaluation\n    num_train_epochs=config['epochs'],                          # Total number of training epochs\n    warmup_steps=500,                            # Number of warmup steps for learning rate scheduler\n    save_total_limit=2,\n    logging_dir=None,                            # Disable logging directory\n    logging_strategy=\"no\",\n    report_to=[]# Limit the total amount of checkpoints`\n\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T06:22:02.422902Z","iopub.execute_input":"2024-09-01T06:22:02.424006Z","iopub.status.idle":"2024-09-01T06:22:02.567235Z","shell.execute_reply.started":"2024-09-01T06:22:02.423963Z","shell.execute_reply":"2024-09-01T06:22:02.566298Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = transformers.Trainer(\n    model=model,                                 # The model to be trained\n    args=training_args,                          # The training arguments, defined above\n    train_dataset=train_ds,                 # The training dataset\n    eval_dataset=valid_ds,                   # The evaluation dataset\n    tokenizer=tokenizer,                         # The tokenizer\n    compute_metrics=compute_metrics, \n    \n)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T06:22:07.253728Z","iopub.execute_input":"2024-09-01T06:22:07.254316Z","iopub.status.idle":"2024-09-01T06:22:22.822393Z","shell.execute_reply.started":"2024-09-01T06:22:07.254277Z","shell.execute_reply":"2024-09-01T06:22:22.821451Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-09-01T06:22:23.566544Z","iopub.execute_input":"2024-09-01T06:22:23.567934Z","iopub.status.idle":"2024-09-01T06:24:19.474458Z","shell.execute_reply.started":"2024-09-01T06:22:23.567888Z","shell.execute_reply":"2024-09-01T06:24:19.473303Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [189/189 01:51, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=189, training_loss=0.565313389692357, metrics={'train_runtime': 114.4895, 'train_samples_per_second': 209.626, 'train_steps_per_second': 1.651, 'total_flos': 249110795520000.0, 'train_loss': 0.565313389692357, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.save_state()","metadata":{"execution":{"iopub.status.busy":"2024-09-01T06:24:38.255104Z","iopub.execute_input":"2024-09-01T06:24:38.255513Z","iopub.status.idle":"2024-09-01T06:24:38.260987Z","shell.execute_reply.started":"2024-09-01T06:24:38.255473Z","shell.execute_reply":"2024-09-01T06:24:38.259998Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"trainer.save_model()","metadata":{"execution":{"iopub.status.busy":"2024-09-01T06:24:41.593029Z","iopub.execute_input":"2024-09-01T06:24:41.593441Z","iopub.status.idle":"2024-09-01T06:24:41.697381Z","shell.execute_reply.started":"2024-09-01T06:24:41.593405Z","shell.execute_reply":"2024-09-01T06:24:41.696305Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}